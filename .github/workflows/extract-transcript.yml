name: Extract Podcast Transcript

on:
  workflow_dispatch:
    inputs:
      specific_file:
        description: 'Specific file to process (leave empty to process all)'
        required: false

# Add permissions to allow writing to the repository
permissions:
  contents: write

jobs:
  extract-transcript:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install lxml beautifulsoup4
      
      - name: Process cache files from data/raw
        run: |
          # Create directories
          mkdir -p cache/extracted
          mkdir -p data/transcripts
          
          # Determine which files to process
          if [ -n "${{ github.event.inputs.specific_file }}" ]; then
            CACHE_FILES=("data/raw/${{ github.event.inputs.specific_file }}")
            echo "Processing specific file: ${CACHE_FILES[0]}"
          else
            CACHE_FILES=(data/raw/*.zip)
            echo "Processing all zip files in data/raw/ directory"
          fi
          
          # Process each zip file
          for zip_file in "${CACHE_FILES[@]}"; do
            if [ -f "$zip_file" ]; then
              echo "Extracting $zip_file"
              # Extract the episode ID from the filename
              EPISODE_ID=$(basename "$zip_file" _cache.zip)
              
              # Create a directory for this specific episode
              mkdir -p "cache/extracted/$EPISODE_ID"
              
              # Extract the zip file
              unzip -o "$zip_file" -d "cache/extracted/$EPISODE_ID"
              
              echo "Extracted contents of $zip_file to cache/extracted/$EPISODE_ID"
            else
              echo "File not found: $zip_file"
            fi
          done
          
          # List the extracted contents to debug
          find cache/extracted -type f | grep -i ttml || echo "No TTML files found"
      
      - name: Process TTML files and extract transcript
        run: |
          python - <<'EOF'
          import os
          import re
          import json
          from bs4 import BeautifulSoup
          from datetime import datetime
          
          # Create data directory if it doesn't exist
          os.makedirs('data/transcripts', exist_ok=True)
          
          # Find all TTML files in the cache directory
          ttml_files = []
          for root, dirs, files in os.walk('cache/extracted'):
              for file in files:
                  if file.endswith('.ttml'):
                      ttml_files.append(os.path.join(root, file))
          
          print(f"Found {len(ttml_files)} TTML files")
          
          # Process each TTML file
          for ttml_file in ttml_files:
              try:
                  print(f"Processing {ttml_file}")
                  
                  # Extract episode ID from the directory structure
                  # Path format: cache/extracted/EPISODE_ID/...
                  path_parts = ttml_file.split(os.sep)
                  episode_id = path_parts[2] if len(path_parts) > 2 else None
                  
                  # Parse the TTML file
                  with open(ttml_file, 'r', encoding='utf-8') as f:
                      soup = BeautifulSoup(f.read(), 'xml')
                  
                  # Extract podcast ID from filename
                  match = re.search(r'(\d+)\.ttml', os.path.basename(ttml_file))
                  if not match:
                      print(f"Could not extract podcast ID from {ttml_file}")
                      continue
                  
                  podcast_id = match.group(1)
                  
                  # Try to extract episode title from the TTML metadata
                  episode_title = None
                  metadata = soup.find('metadata')
                  if metadata:
                      title_elem = metadata.find('title')
                      if title_elem and title_elem.text:
                          episode_title = title_elem.text.strip()
                  
                  if not episode_title:
                      episode_title = f"Episode {episode_id}"
                  
                  # Extract transcript content
                  transcript_chunks = []
                  speaking_chunks = soup.select('p')
                  
                  for chunk in speaking_chunks:
                      speaker = chunk.get('ttm:agent', 'Unknown')
                      
                      # Extract time information (begin and end attributes)
                      begin_time = chunk.get('begin', '')
                      end_time = chunk.get('end', '')
                      
                      # Convert time format to seconds if available
                      begin_seconds = 0
                      end_seconds = 0
                      duration_seconds = 0
                      
                      if begin_time:
                          # TTML time format is typically HH:MM:SS.sss
                          try:
                              time_parts = begin_time.replace(':', '.').split('.')
                              if len(time_parts) >= 3:
                                  hours = int(time_parts[0]) if time_parts[0] else 0
                                  minutes = int(time_parts[1]) if time_parts[1] else 0
                                  seconds = float('.'.join(time_parts[2:]))
                                  begin_seconds = hours * 3600 + minutes * 60 + seconds
                          except Exception as e:
                              print(f"Error parsing begin time {begin_time}: {e}")
                      
                      if end_time:
                          try:
                              time_parts = end_time.replace(':', '.').split('.')
                              if len(time_parts) >= 3:
                                  hours = int(time_parts[0]) if time_parts[0] else 0
                                  minutes = int(time_parts[1]) if time_parts[1] else 0
                                  seconds = float('.'.join(time_parts[2:]))
                                  end_seconds = hours * 3600 + minutes * 60 + seconds
                          except Exception as e:
                              print(f"Error parsing end time {end_time}: {e}")
                      
                      if begin_seconds and end_seconds:
                          duration_seconds = end_seconds - begin_seconds
                      
                      # Extract sentences
                      sentences = []
                      for sentence in chunk.select('span[podcasts\\:unit="sentence"]'):
                          text = ' '.join([span.text for span in sentence.select('span')])
                          sentences.append(text)
                      
                      # Join sentences
                      text = ' '.join(sentences)
                      
                      if text.strip():  # Only add non-empty chunks
                          transcript_chunk = {
                              'speaker': speaker,
                              'text': text
                          }
                          
                          # Add time information if available
                          if begin_time:
                              transcript_chunk['begin_time'] = begin_time
                              transcript_chunk['begin_seconds'] = begin_seconds
                          if end_time:
                              transcript_chunk['end_time'] = end_time
                              transcript_chunk['end_seconds'] = end_seconds
                          if begin_seconds and end_seconds:
                              transcript_chunk['duration_seconds'] = duration_seconds
                          
                          transcript_chunks.append(transcript_chunk)
                  
                  # Create a safe filename using episode_id
                  if episode_id:
                      output_file = f"data/transcripts/{episode_id}_transcript.json"
                  else:
                      # Fallback to a safe title derived from the episode title
                      safe_title = re.sub(r'[^\w\-\. ]', '_', episode_title)
                      output_file = f"data/transcripts/{safe_title}_transcript.json"
                      print("Warning: Using title-based filename because episode_id is not available")
                  
                  # Get the current date for metadata
                  current_date = datetime.now().isoformat()
                  
                  with open(output_file, 'w', encoding='utf-8') as f:
                      json.dump({
                          'episode_title': episode_title,
                          'podcast_id': podcast_id,
                          'episode_id': episode_id,
                          'extracted_date': current_date,
                          'transcript': transcript_chunks
                      }, f, indent=2, ensure_ascii=False)
                  
                  print(f"Saved transcript to {output_file}")
                  
              except Exception as e:
                  print(f"Error processing {ttml_file}: {e}")
                  import traceback
                  traceback.print_exc()
          EOF
      
      - name: Commit transcript data
        run: |
          git config --global user.name 'GitHub Actions'
          git config --global user.email 'actions@github.com'
          git add data/transcripts/
          git commit -m "Add transcripts from data/raw cache files" || echo "No changes to commit"
          git push