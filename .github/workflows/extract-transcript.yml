name: Extract Podcast Transcript

on:
  workflow_dispatch:
    inputs:
      specific_file:
        description: 'Specific file to process (leave empty to process all)'
        required: false

# Add permissions to allow writing to the repository
permissions:
  contents: write

jobs:
  extract-transcript:
    runs-on: ubuntu-latest
    
    # Add environment variables for Spotify API
    env:
      SPOTIFY_CLIENT_ID: ${{ secrets.SPOTIFY_CLIENT_ID }}
      SPOTIFY_CLIENT_SECRET: ${{ secrets.SPOTIFY_CLIENT_SECRET }}
    
    steps:
      - name: Set up error handling and debugging
        run: |
          set -euo pipefail
          set -x
      
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r scripts/requirements.txt
      
      - name: Lint with flake8
        run: |
          python -m flake8 scripts/ --max-line-length=120
      
      - name: Check formatting with black
        run: |
          python -m black --check scripts/
      
      - name: Process cache files from data/raw
        run: |
          # Create directories
          mkdir -p cache/extracted
          mkdir -p data/transcripts
          
          # Determine which files to process
          if [ -n "${{ github.event.inputs.specific_file }}" ]; then
            CACHE_FILES=("data/raw/${{ github.event.inputs.specific_file }}")
            echo "Processing specific file: ${CACHE_FILES[0]}"
          else
            CACHE_FILES=(data/raw/*.zip)
            echo "Processing all zip files in data/raw/ directory"
          fi
          
          # Process each zip file
          for zip_file in "${CACHE_FILES[@]}"; do
            if [ -f "$zip_file" ]; then
              echo "Extracting $zip_file"
              # Extract the episode ID from the filename
              EPISODE_ID=$(basename "$zip_file" _cache.zip)
              
              # Create a directory for this specific episode
              mkdir -p "cache/extracted/$EPISODE_ID"
              
              # Extract the zip file
              unzip -o "$zip_file" -d "cache/extracted/$EPISODE_ID"
              
              echo "Extracted contents of $zip_file to cache/extracted/$EPISODE_ID"
            else
              echo "File not found: $zip_file"
            fi
          done
          
          # List the extracted contents to debug
          find cache/extracted -type f | grep -i ttml || echo "No TTML files found"
      
      - name: Process TTML files and extract transcript
        run: |
          python - <<'EOF'
          import os
          import re
          import json
          from bs4 import BeautifulSoup
          from datetime import datetime
          import logging
          
          # Set up logging
          logging.basicConfig(
              level=logging.INFO,
              format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
          )
          logger = logging.getLogger('extract_transcript')
          
          # Create data directory if it doesn't exist
          os.makedirs('data/transcripts', exist_ok=True)
          
          primary_id_for_filename = os.environ.get('EPISODE_ID_FROM_ZIP')
          
          # Load episode links data for titles and release dates if available
          episode_metadata = {}
          episode_links_path = 'data/episodes/episode_links.json'
          
          if os.path.exists(episode_links_path):
              try:
                  with open(episode_links_path, 'r', encoding='utf-8') as f:
                      episodes_data = json.load(f)
                      
                      # Create a lookup dictionary by apple_id
                      for episode in episodes_data:
                          # Episodes may now have apple_id instead of episode_id
                          apple_id = str(episode.get('apple_id', ''))
                          if apple_id:
                              episode_metadata[apple_id] = {
                                  'title': episode.get('title'),
                                  'release_date': episode.get('release_date'),
                                  'spotify_id': episode.get('spotify_id')  # Get spotify_id from combined data
                              }
                  logger.info(f"Loaded metadata for {len(episode_metadata)} episodes")
              except Exception as e:
                  logger.error(f"Error loading episode links: {e}")
          else:
              logger.warning(f"Warning: Episode links file not found at {episode_links_path}")
          
          # Find all TTML files in the cache directory
          ttml_files = []
          for root, dirs, files in os.walk('cache/extracted'):
              for file in files:
                  if file.endswith('.ttml'):
                      ttml_files.append(os.path.join(root, file))
          
          logger.info(f"Found {len(ttml_files)} TTML files")
          
          # Process each TTML file
          for ttml_file in ttml_files:
              try:
                  logger.info(f"Processing {ttml_file}")
                  
                  # Extract episode ID from the directory structure
                  # Path format: cache/extracted/EPISODE_ID/...
                  path_parts = ttml_file.split(os.sep)
                  episode_id = path_parts[2] if len(path_parts) > 2 else None
                  
                  # Parse the TTML file
                  with open(ttml_file, 'r', encoding='utf-8') as f:
                      soup = BeautifulSoup(f.read(), 'xml')
                  
                  # Extract podcast ID from filename
                  match = re.search(r'(\d+)\.ttml', os.path.basename(ttml_file))
                  if not match:
                      logger.warning(f"Could not extract podcast ID from {ttml_file}")
                      continue
                  
                  apple_id = match.group(1)
                  
                  # Get episode title and Spotify ID from metadata
                  episode_title = None
                  spotify_id = None
                  upload_date = None
                  
                  # First check if we have metadata from episode_links.json
                  if apple_id in episode_metadata:
                      meta = episode_metadata[apple_id]
                      episode_title = meta.get('title')
                      spotify_id = meta.get('spotify_id')
                      upload_date = meta.get('release_date')
                  
                  # If no title from metadata, try to get it from TTML
                  if not episode_title:
                      metadata = soup.find('metadata')
                      if metadata:
                          title_elem = metadata.find('title')
                          if title_elem and title_elem.text:
                              episode_title = title_elem.text.strip()
                  
                  # Final fallback for title
                  if not episode_title:
                      episode_title = f"Episode {episode_id}"
                  
                  # Extract transcript content
                  transcript_chunks = []
                  speaking_chunks = soup.select('p')
                  
                  for chunk in speaking_chunks:
                      speaker = chunk.get('ttm:agent', 'Unknown')
                      
                      # Extract time information (begin attribute only)
                      begin_time = chunk.get('begin', '')
                      
                      # Convert time format to seconds if available
                      begin_seconds = 0
                      
                      if begin_time:
                          # TTML time format is typically HH:MM:SS.sss
                          try:
                              # First handle the case where we have hours
                              if ':' in begin_time:
                                  time_parts = begin_time.split(':')
                                  hours = int(time_parts[0]) if len(time_parts) > 2 else 0
                                  minutes = int(time_parts[-2]) if len(time_parts) > 1 else 0
                                  
                                  # For seconds, handle milliseconds properly
                                  seconds_parts = time_parts[-1].split('.')
                                  seconds = int(seconds_parts[0])
                                  milliseconds = int(seconds_parts[1]) if len(seconds_parts) > 1 else 0
                                  
                                  begin_seconds = hours * 3600 + minutes * 60 + seconds + (milliseconds / 1000)
                              else:
                                  # Handle simple seconds.milliseconds format
                                  seconds_parts = begin_time.split('.')
                                  seconds = int(seconds_parts[0])
                                  milliseconds = int(seconds_parts[1]) if len(seconds_parts) > 1 else 0
                                  begin_seconds = seconds + (milliseconds / 1000)
                          except Exception as e:
                              logger.error(f"Error parsing begin time {begin_time}: {e}")
                      
                      # Round down seconds to integers
                      begin_seconds = int(begin_seconds)
                      
                      # Extract sentences
                      sentences = []
                      for sentence in chunk.select('span[podcasts\\:unit="sentence"]'):
                          text = ' '.join([span.text for span in sentence.select('span')])
                          sentences.append(text)
                      
                      # Join sentences
                      text = ' '.join(sentences)
                      
                      if text.strip():  # Only add non-empty chunks
                          transcript_chunk = {
                              'speaker': speaker,
                              'text': text,
                              'begin_seconds': begin_seconds
                          }
                          
                          transcript_chunks.append(transcript_chunk)
                  
                  # Create a safe filename using episode_id
                  if episode_id:
                      output_file = f"data/transcripts/{episode_id}_transcript.json"
                  else:
                      # Fallback to a safe title derived from the episode title
                      safe_title = re.sub(r'[^\w\-\. ]', '_', episode_title)
                      output_file = f"data/transcripts/{safe_title}_transcript.json"
                      logger.warning("Using title-based filename because episode_id is not available")
                  
                  # Create the output data structure
                  output_data = {
                      "episode_title": episode_title,
                      "apple_id": apple_id,
                      "filename_primary_id": episode_id, # This episode_id is from path_parts[2]
                  }
                  
                  # Add Spotify ID if available
                  if spotify_id:
                      output_data["spotify_id"] = spotify_id
                  
                  # Add upload date if available
                  if upload_date:
                      output_data["upload_date"] = upload_date
                  
                  # Add the transcript
                  output_data["transcript"] = transcript_chunks
                  
                  with open(output_file, 'w', encoding='utf-8') as f:
                      json.dump(output_data, f, indent=2, ensure_ascii=False)
                  
                  logger.info(f"Saved transcript to {output_file}")
                  
              except Exception as e:
                  logger.error(f"Error processing {ttml_file}: {e}")
                  import traceback
                  traceback.print_exc()
          EOF
      
      - name: Commit transcript data
        run: |
          set -euo pipefail
          set -x
          git config --global user.name 'GitHub Actions'
          git config --global user.email 'actions@github.com'
          if git status --porcelain data/transcripts | grep -q .; then
            git pull --rebase origin main
            git add data/transcripts/
            git commit -m "Update transcripts with IDs and rounded seconds"
            git push
          else
            echo "No changes to commit"