name: Extract Podcast Transcript

on:
  workflow_dispatch:
    inputs:
      specific_file:
        description: 'Specific file to process (leave empty to process all)'
        required: false

# Add permissions to allow writing to the repository
permissions:
  contents: write

jobs:
  extract-transcript:
    runs-on: ubuntu-latest
    
    # Add environment variables for Spotify API
    env:
      SPOTIFY_CLIENT_ID: ${{ secrets.SPOTIFY_CLIENT_ID }}
      SPOTIFY_CLIENT_SECRET: ${{ secrets.SPOTIFY_CLIENT_SECRET }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install lxml beautifulsoup4 requests
      
      - name: Process cache files from data/raw
        run: |
          # Create directories
          mkdir -p cache/extracted
          mkdir -p data/transcripts
          
          # Determine which files to process
          if [ -n "${{ github.event.inputs.specific_file }}" ]; then
            CACHE_FILES=("data/raw/${{ github.event.inputs.specific_file }}")
            echo "Processing specific file: ${CACHE_FILES[0]}"
          else
            CACHE_FILES=(data/raw/*.zip)
            echo "Processing all zip files in data/raw/ directory"
          fi
          
          # Process each zip file
          for zip_file in "${CACHE_FILES[@]}"; do
            if [ -f "$zip_file" ]; then
              echo "Extracting $zip_file"
              # Extract the episode ID from the filename
              EPISODE_ID=$(basename "$zip_file" _cache.zip)
              
              # Create a directory for this specific episode
              mkdir -p "cache/extracted/$EPISODE_ID"
              
              # Extract the zip file
              unzip -o "$zip_file" -d "cache/extracted/$EPISODE_ID"
              
              echo "Extracted contents of $zip_file to cache/extracted/$EPISODE_ID"
            else
              echo "File not found: $zip_file"
            fi
          done
          
          # List the extracted contents to debug
          find cache/extracted -type f | grep -i ttml || echo "No TTML files found"
      
      - name: Process TTML files and extract transcript
        run: |
          python - <<'EOF'
          import os
          import re
          import json
          from bs4 import BeautifulSoup
          from datetime import datetime
          import requests
          
          # Create data directory if it doesn't exist
          os.makedirs('data/transcripts', exist_ok=True)
          
          # Load episode links data for titles and release dates if available
          episode_metadata = {}
          episode_links_path = 'data/episodes/episode_links.json'
          
          if os.path.exists(episode_links_path):
              try:
                  with open(episode_links_path, 'r', encoding='utf-8') as f:
                      episodes_data = json.load(f)
                      
                      # Create a lookup dictionary by episode_id
                      for episode in episodes_data:
                          episode_id = str(episode.get('episode_id'))
                          if episode_id:
                              episode_metadata[episode_id] = {
                                  'title': episode.get('title'),
                                  'release_date': episode.get('release_date')
                              }
                  print(f"Loaded metadata for {len(episode_metadata)} episodes")
              except Exception as e:
                  print(f"Error loading episode links: {e}")
          else:
              print(f"Warning: Episode links file not found at {episode_links_path}")
          
          # Function to get Spotify ID for an episode based on release date and title
          def get_spotify_id(title, upload_date):
              try:
                  # Gegenwartscheck podcast Spotify show ID
                  show_id = "3sObg5LiElhdkjCVGYyuGG"  # Gegenwartscheck on Spotify
                  
                  # Get Spotify access token (using client credentials flow)
                  # In a real workflow, you'd store these securely in GitHub Secrets
                  # This is a simplified example - in production, use GitHub secrets
                  client_id = os.environ.get('SPOTIFY_CLIENT_ID')
                  client_secret = os.environ.get('SPOTIFY_CLIENT_SECRET')
                  
                  if not client_id or not client_secret:
                      print("Missing Spotify credentials, skipping Spotify ID lookup")
                      return None
                  
                  auth_response = requests.post(
                      'https://accounts.spotify.com/api/token',
                      data={
                          'grant_type': 'client_credentials',
                          'client_id': client_id,
                          'client_secret': client_secret,
                      }
                  )
                  
                  if auth_response.status_code != 200:
                      print(f"Failed to get Spotify token: {auth_response.status_code}")
                      return None
                      
                  auth_data = auth_response.json()
                  access_token = auth_data['access_token']
                  
                  # Use the token to call the Spotify API
                  headers = {
                      'Authorization': f'Bearer {access_token}'
                  }
                  
                  # Get episodes for the show (paginate if needed)
                  response = requests.get(
                      f'https://api.spotify.com/v1/shows/{show_id}/episodes?limit=50',
                      headers=headers
                  )
                  
                  if response.status_code != 200:
                      print(f"Failed to get Spotify episodes: {response.status_code}")
                      return None
                      
                  episodes = response.json()
                  
                  # Search for matching episode based on release date
                  for episode in episodes['items']:
                      episode_date = episode['release_date']  # Format: YYYY-MM-DD
                      if episode_date == upload_date:
                          # We found a date match, now check title similarity
                          # For simplicity, we're using a basic contains check
                          if title.lower() in episode['name'].lower() or episode['name'].lower() in title.lower():
                              return episode['id']
                  
                  print(f"No matching Spotify episode found for {title} ({upload_date})")
                  return None
              except Exception as e:
                  print(f"Error getting Spotify ID: {e}")
                  return None
          
          # Find all TTML files in the cache directory
          ttml_files = []
          for root, dirs, files in os.walk('cache/extracted'):
              for file in files:
                  if file.endswith('.ttml'):
                      ttml_files.append(os.path.join(root, file))
          
          print(f"Found {len(ttml_files)} TTML files")
          
          # Process each TTML file
          for ttml_file in ttml_files:
              try:
                  print(f"Processing {ttml_file}")
                  
                  # Extract episode ID from the directory structure
                  # Path format: cache/extracted/EPISODE_ID/...
                  path_parts = ttml_file.split(os.sep)
                  episode_id = path_parts[2] if len(path_parts) > 2 else None
                  
                  # Parse the TTML file
                  with open(ttml_file, 'r', encoding='utf-8') as f:
                      soup = BeautifulSoup(f.read(), 'xml')
                  
                  # Extract podcast ID from filename
                  match = re.search(r'(\d+)\.ttml', os.path.basename(ttml_file))
                  if not match:
                      print(f"Could not extract podcast ID from {ttml_file}")
                      continue
                  
                  podcast_id = match.group(1)
                  
                  # Get episode title from metadata or fallback to TTML
                  episode_title = None
                  upload_date = None
                  
                  # First check if we have metadata from episode_links.json
                  if episode_id in episode_metadata:
                      episode_title = episode_metadata[episode_id].get('title')
                      
                      # Extract just the date part from the release date (format: 2021-04-12T07:00:00Z)
                      release_date_str = episode_metadata[episode_id].get('release_date')
                      if release_date_str:
                          try:
                              # Extract just the date part
                              upload_date = release_date_str.split('T')[0]
                          except Exception as e:
                              print(f"Error parsing release date {release_date_str}: {e}")
                  
                  # If no title from metadata, try to get it from TTML
                  if not episode_title:
                      metadata = soup.find('metadata')
                      if metadata:
                          title_elem = metadata.find('title')
                          if title_elem and title_elem.text:
                              episode_title = title_elem.text.strip()
                  
                  # Final fallback for title
                  if not episode_title:
                      episode_title = f"Episode {episode_id}"
                  
                  # Get Spotify ID if we have upload date 
                  spotify_id = None
                  if upload_date and episode_title:
                      spotify_id = get_spotify_id(episode_title, upload_date)
                      
                  # Extract transcript content
                  transcript_chunks = []
                  speaking_chunks = soup.select('p')
                  
                  for chunk in speaking_chunks:
                      speaker = chunk.get('ttm:agent', 'Unknown')
                      
                      # Extract time information (begin attribute only)
                      begin_time = chunk.get('begin', '')
                      
                      # Convert time format to seconds if available
                      begin_seconds = 0
                      
                      if begin_time:
                          # TTML time format is typically HH:MM:SS.sss
                          try:
                              # First handle the case where we have hours
                              if ':' in begin_time:
                                  time_parts = begin_time.split(':')
                                  hours = int(time_parts[0]) if len(time_parts) > 2 else 0
                                  minutes = int(time_parts[-2]) if len(time_parts) > 1 else 0
                                  
                                  # For seconds, handle milliseconds properly
                                  seconds_parts = time_parts[-1].split('.')
                                  seconds = int(seconds_parts[0])
                                  milliseconds = int(seconds_parts[1]) if len(seconds_parts) > 1 else 0
                                  
                                  begin_seconds = hours * 3600 + minutes * 60 + seconds + (milliseconds / 1000)
                              else:
                                  # Handle simple seconds.milliseconds format
                                  seconds_parts = begin_time.split('.')
                                  seconds = int(seconds_parts[0])
                                  milliseconds = int(seconds_parts[1]) if len(seconds_parts) > 1 else 0
                                  begin_seconds = seconds + (milliseconds / 1000)
                          except Exception as e:
                              print(f"Error parsing begin time {begin_time}: {e}")
                      
                      # Round down seconds to integers
                      begin_seconds = int(begin_seconds)
                      
                      # Extract sentences
                      sentences = []
                      for sentence in chunk.select('span[podcasts\\:unit="sentence"]'):
                          text = ' '.join([span.text for span in sentence.select('span')])
                          sentences.append(text)
                      
                      # Join sentences
                      text = ' '.join(sentences)
                      
                      if text.strip():  # Only add non-empty chunks
                          transcript_chunk = {
                              'speaker': speaker,
                              'text': text,
                              'begin_seconds': begin_seconds
                          }
                          
                          transcript_chunks.append(transcript_chunk)
                  
                  # Create a safe filename using episode_id
                  if episode_id:
                      output_file = f"data/transcripts/{episode_id}_transcript.json"
                  else:
                      # Fallback to a safe title derived from the episode title
                      safe_title = re.sub(r'[^\w\-\. ]', '_', episode_title)
                      output_file = f"data/transcripts/{safe_title}_transcript.json"
                      print("Warning: Using title-based filename because episode_id is not available")
                  
                  # Create the output data structure
                  output_data = {
                      "episode_title": episode_title,
                      "apple_id": podcast_id,  # Renamed from podcast_id to apple_id
                      "episode_id": episode_id,
                  }
                  
                  # Add Spotify ID if available
                  if spotify_id:
                      output_data["spotify_id"] = spotify_id
                  
                  # Add upload date if available
                  if upload_date:
                      output_data["upload_date"] = upload_date
                  
                  # Add the transcript
                  output_data["transcript"] = transcript_chunks
                  
                  with open(output_file, 'w', encoding='utf-8') as f:
                      json.dump(output_data, f, indent=2, ensure_ascii=False)
                  
                  print(f"Saved transcript to {output_file}")
                  
              except Exception as e:
                  print(f"Error processing {ttml_file}: {e}")
                  import traceback
                  traceback.print_exc()
          EOF
      
      - name: Commit transcript data
        run: |
          git config --global user.name 'GitHub Actions'
          git config --global user.email 'actions@github.com'
          git add data/transcripts/
          git commit -m "Update transcripts with IDs and rounded seconds" || echo "No changes to commit"
          git push origin main