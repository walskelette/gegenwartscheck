name: Get Podcast Episode Links

on:
  workflow_dispatch:
  schedule:
    - cron: '0 0 * * 0'  # Run weekly on Sunday at midnight

permissions:
  contents: write

jobs:
  fetch-episodes:
    runs-on: ubuntu-latest
    outputs:
      episode_links: ${{ steps.get-links.outputs.episode_links }}
      spotify_episode_links: ${{ steps.get-spotify-links.outputs.spotify_episode_links }}
      spotify_episode_count: ${{ steps.get-spotify-links.outputs.spotify_episode_count }}
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests
      
      - name: Fetch podcast episodes from Apple
        id: get-links
        run: |
          python - <<'EOF'
          import requests
          import json
          import os
          import logging
          
          # Configure logging
          logging.basicConfig(
              level=logging.INFO,
              format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
          )
          logger = logging.getLogger('apple_podcast_fetch')
          
          # Dies sogenannte Gegenwart podcast ID
          podcast_id = '1522895163'
          
          # Fetch all episodes by making multiple requests if needed
          episodes = []
          limit = 100  # Apple API allows up to 100 episodes per request
          offset = 0
          
          logger.info(f"Starting fetch from Apple Podcasts for podcast ID: {podcast_id}")
          
          while True:
              url = f'https://itunes.apple.com/lookup?id={podcast_id}&entity=podcastEpisode&limit={limit}&offset={offset}'
              logger.info(f"Making request to Apple API with offset {offset}")
              
              try:
                  response = requests.get(url)
                  response.raise_for_status()
                  data = response.json()
                  
                  new_episodes = []
                  for episode in data.get('results', []):
                      # Only include items with wrapperType == 'podcastEpisode', which filters out the podcast itself
                      if episode.get('wrapperType') == 'podcastEpisode':
                          episode_title = episode.get('trackName')
                          episode_url = episode.get('trackViewUrl')
                          episode_id = episode.get('trackId')
                          release_date = episode.get('releaseDate')
                          
                          # Process the release date to remove the time part
                          if release_date and 'T' in release_date:
                              release_date_date_only = release_date.split('T')[0]
                          else:
                              release_date_date_only = release_date
                          
                          # Include all episodes, not just those with "gegenwartscheck" in the title
                          new_episodes.append({
                              "title": episode_title,
                              "url": episode_url,
                              "apple_id": episode_id,
                              "release_date": release_date_date_only,
                              "source": "apple"
                          })
                  
                  logger.info(f"Retrieved {len(new_episodes)} episodes from Apple (offset {offset})")
                  
                  if not new_episodes:
                      logger.info("No more episodes to fetch from Apple")
                      break
                  
                  episodes.extend(new_episodes)
                  offset += limit
                  
                  # If we got fewer results than the limit, we've reached the end
                  if len(new_episodes) < limit:
                      logger.info("Reached the end of available episodes from Apple")
                      break
              except Exception as e:
                  logger.error(f"Error fetching from Apple: {str(e)}")
                  break
          
          logger.info(f"Found total of {len(episodes)} episodes from Apple")
          
          # Create data directory if it doesn't exist
          os.makedirs('data/episodes', exist_ok=True)
          
          # Save to file in data directory
          with open('data/episodes/apple_episode_links.json', 'w') as f:
              json.dump(episodes, f, indent=2)
          
          # Set output for next workflow
          episode_links_json = json.dumps(episodes)
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"episode_links={episode_links_json}\n")
          
          print(f"Found {len(episodes)} episodes from Apple:")
          for ep in episodes[:10]:  # Print first 10 episodes for brevity
              print(f"- {ep['title']}: {ep['url']}")
          print("...and more")
          EOF
      
      - name: Fetch podcast episodes from Spotify
        id: get-spotify-links
        env:
          SPOTIFY_CLIENT_ID: ${{ secrets.SPOTIFY_CLIENT_ID }}
          SPOTIFY_CLIENT_SECRET: ${{ secrets.SPOTIFY_CLIENT_SECRET }}
          SPOTIFY_SHOW_ID: '09j4UbTqLqpaUr2F7pxIgl'  # "Die sogenannte Gegenwart" Spotify ID
        run: |
          python scripts/spotify_fetch.py
      
      - name: Merge episode data
        run: |
          python - <<'EOF'
          import json
          import os
          import logging
          from datetime import datetime
          
          # Configure logging
          logging.basicConfig(
              level=logging.INFO,
              format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
          )
          logger = logging.getLogger('merge_episodes')
          
          # Load episodes from both sources
          try:
              with open('data/episodes/apple_episode_links.json', 'r') as f:
                  apple_episodes = json.load(f)
                  logger.info(f"Loaded {len(apple_episodes)} episodes from Apple")
          except Exception as e:
              logger.error(f"Error loading Apple episodes: {str(e)}")
              apple_episodes = []
          
          try:
              # Check if file exists before trying to load
              spotify_path = 'data/episodes/spotify_episode_links.json'
              if os.path.exists(spotify_path):
                  with open(spotify_path, 'r') as f:
                      spotify_episodes = json.load(f)
                      logger.info(f"Loaded {len(spotify_episodes)} episodes from Spotify")
              else:
                  logger.warning(f"Spotify episodes file not found: {spotify_path}")
                  spotify_episodes = []
          except Exception as e:
              logger.error(f"Error loading Spotify episodes: {str(e)}")
              spotify_episodes = []
          
          # Create a dictionary to group episodes by release date
          episodes_by_date = {}
          
          # Process Apple episodes
          for episode in apple_episodes:
              release_date = episode.get('release_date')
              if not release_date:
                  logger.warning(f"Skipping Apple episode with no release date: {episode.get('title')}")
                  continue
                  
              # Create or update the entry for this date
              if release_date not in episodes_by_date:
                  episodes_by_date[release_date] = {
                      'title': episode.get('title'),
                      'release_date': release_date,
                      'apple_id': episode.get('apple_id'),
                      'apple_url': episode.get('url')
                  }
              else:
                  # If this date already exists, update Apple info
                  episodes_by_date[release_date]['apple_id'] = episode.get('apple_id')
                  episodes_by_date[release_date]['apple_url'] = episode.get('url')
                  
                  # Only update title if we don't already have one
                  if not episodes_by_date[release_date].get('title'):
                      episodes_by_date[release_date]['title'] = episode.get('title')
          
          # Process Spotify episodes
          for episode in spotify_episodes:
              release_date = episode.get('release_date')
              if not release_date:
                  logger.warning(f"Skipping Spotify episode with no release date: {episode.get('title')}")
                  continue
                  
              # Make sure the release date is just the date part (YYYY-MM-DD)
              if 'T' in release_date:
                  release_date = release_date.split('T')[0]
              
              # Create or update the entry for this date
              if release_date not in episodes_by_date:
                  episodes_by_date[release_date] = {
                      'title': episode.get('title'),
                      'release_date': release_date,
                      'spotify_id': episode.get('episode_id'),
                      'spotify_url': episode.get('url')
                  }
              else:
                  # If this date already exists, update Spotify info
                  episodes_by_date[release_date]['spotify_id'] = episode.get('episode_id')
                  episodes_by_date[release_date]['spotify_url'] = episode.get('url')
                  
                  # Only update title if we don't already have one
                  if not episodes_by_date[release_date].get('title'):
                      episodes_by_date[release_date]['title'] = episode.get('title')
          
          # Convert the dictionary to a list
          merged_episodes = list(episodes_by_date.values())
          
          # Sort by release date (newest first)
          merged_episodes.sort(key=lambda x: x.get('release_date', ''), reverse=True)
          
          logger.info(f"Combined total: {len(merged_episodes)} unique episodes by date")
          
          # Save combined episodes
          with open('data/episodes/episode_links.json', 'w') as f:
              json.dump(merged_episodes, f, indent=2)
          
          logger.info("Successfully saved combined episodes to data/episodes/episode_links.json")
          print(f"Combined {len(merged_episodes)} unique episodes from Apple and Spotify")
          EOF
          
      - name: Upload episode links as artifact
        uses: actions/upload-artifact@v4
        with:
          name: episode-links
          path: data/episodes/episode_links.json
          
      - name: Commit episode links to repository
        run: |
          git config --global user.name 'GitHub Actions'
          git config --global user.email 'actions@github.com'
          git add data/episodes/apple_episode_links.json
          git add data/episodes/spotify_episode_links.json || echo "Spotify episode links file not available"
          git add data/episodes/episode_links.json
          git commit -m "Update episode links from Apple and Spotify" || echo "No changes to commit"
          git push
